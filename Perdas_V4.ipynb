{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f3944f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação de libs \"Padrão\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta  \n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import workdays as wds\n",
    "from filecmp import cmp\n",
    "import openpyxl\n",
    "from filecmp import dircmp\n",
    "from tqdm import tqdm\n",
    "import xlrd\n",
    "import win32com.client\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# lib para ignorar warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccae49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = r'C:\\POWERBI\\Perdas\\Bases\\\\'\n",
    "pout = r'C:\\POWERBI\\Perdas\\StarSchema\\\\'\n",
    "pval = r'C:\\POWERBI\\Perdas\\Valid\\\\'\n",
    " \n",
    "list_path = [pin, pout, pval]\n",
    " \n",
    "for path in list_path:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3c2a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório para captura das bases necessárias\n",
    "diretorio = r'C:\\\\POWERBI\\\\Perdas\\\\Bases'\n",
    "nome_arquivo = 'PENDENCIAS CONCILIACAO (DEME) 21 02 2025.xlsx'\n",
    "nome_arquivo2 = 'P16-Back_3_202501.csv'\n",
    "diretorio_cambio = r'C:\\\\POWERBI\\\\CambioPronto\\\\Bases\\\\Base_Consolidada'\n",
    "nome_arquivo3 = 'hq_online_offline.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf0b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo Excel carregado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Definir o diretório e o nome do arquivo\n",
    "caminho_arquivo = os.path.join(diretorio, nome_arquivo)\n",
    "\n",
    "# Verificar se o arquivo existe\n",
    "if os.path.exists(caminho_arquivo):\n",
    "    # Carregar o arquivo Excel em um DataFrame, pulando as primeiras 4 linhas\n",
    "    df = pd.read_excel(caminho_arquivo, engine='openpyxl', header=4)\n",
    "    print(\"Arquivo Excel carregado com sucesso.\")\n",
    "else:\n",
    "    print(\"Arquivo Excel não encontrado.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bd52fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletar as colunas especificadas, se existirem\n",
    "colunas_para_deletar = [\"Produto\", \"Cuenta\", \"Área\", \"Superintendência\", \"superintendência         \", \"superintendência\", \"Referencia_3\"]\n",
    "colunas_existentes = [col for col in colunas_para_deletar if col in df.columns]\n",
    "df.drop(columns=colunas_existentes, inplace=True)\n",
    "\n",
    "# Filtrar as linhas onde a coluna \"Justificativa\" inicia com \"DashPerdas\"\n",
    "if 'Justificativa' in df.columns:\n",
    "    df = df[df['Justificativa'].str.startswith('DashPerdas', na=False)]\n",
    "else:\n",
    "    print(\"A coluna 'Justificativa' não existe no DataFrame.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cfc790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normatizar todas as colunas removendo espaços em branco e substituindo por underscores\n",
    "df.columns = df.columns.str.strip().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e748ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a nova ordem das colunas\n",
    "new_order = [\n",
    "    'Referencia_1', 'Referencia_2', 'GIN', 'referencia_tratada', 'Referencia_4', \n",
    "    'Setor', 'Valor', 'Área', 'Moeda', 'Operacao', 'Origem', 'Agente', 'Data_da_Pendência', \n",
    "    'Dias', 'Justificativa', 'status'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad3011b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar as linhas onde a coluna \"Justificativa\" inicia com \"DashPerdas\"\n",
    "if 'Justificativa' in df.columns:\n",
    "    df = df[df['Justificativa'].str.startswith('DashPerdas', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76525b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluir nova coluna \"referencia_tratada\" e aplicar a regra na coluna \"Referencia_1\"\n",
    "if 'Referencia_1' in df.columns:\n",
    "    df['referencia_tratada'] = df['Referencia_1'].apply(\n",
    "        lambda x: x[4:-3] if isinstance(x, str) and x[:2] in ['FV', 'IM', 'VF', 'IF'] else x\n",
    "    )\n",
    "else:\n",
    "    print(\"A coluna 'Referencia_1' não existe no DataFrame.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e7be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se todas as colunas da nova ordem existem no DataFrame\n",
    "colunas_existentes = [col for col in new_order if col in df.columns]\n",
    "\n",
    "# Reorganizar as colunas\n",
    "df = df.reindex(columns=colunas_existentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42be3d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificar a existência das colunas necessárias para o cruzamento\n",
    "# if 'referencia_tratada' in df.columns and 'NumeroBacen' in df_cambio.columns:\n",
    "#     # Converter as colunas para o mesmo tipo (string)\n",
    "#     df['referencia_tratada'] = df['referencia_tratada'].astype(str)\n",
    "    \n",
    "#     # Adicionar novas colunas ao DataFrame principal\n",
    "#     novas_colunas = [\"Recuperado_(S/N)\", \"Valor_recuperado\", \"Contabilizado_em_perdas_(S/N)\", \"Valor_de_perda\"]\n",
    "#     for coluna in novas_colunas:\n",
    "#         df[coluna] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e7edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV carregado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Definir o caminho do arquivo CSV\n",
    "caminho_csv = os.path.join(diretorio_cambio, nome_arquivo3)\n",
    "\n",
    "# Verificar se o arquivo CSV existe\n",
    "if os.path.exists(caminho_csv):\n",
    "    try:\n",
    "        # Carregar o arquivo CSV em um DataFrame, especificando o delimitador correto\n",
    "        df_cambio = pd.read_csv(caminho_csv, sep=';', encoding='utf-8-sig')\n",
    "        print(\"Arquivo CSV carregado com sucesso.\")\n",
    "\n",
    "        # Normatizar todas as colunas removendo espaços em branco\n",
    "        df_cambio.columns = df_cambio.columns.str.strip()\n",
    "        \n",
    "        # Reduzir as colunas para as necessárias\n",
    "        colunas_necessarias = [\n",
    "            'GrpTpPessoa', 'Segmento', 'SegmentoGrupo', \n",
    "            'NomeBeneficiarioExterior', 'NomeClienteCustodiante', \n",
    "            'NumeroBacen', 'NumeroPreContrato'\n",
    "        ]\n",
    "        df_cambio = df_cambio[colunas_necessarias]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar o arquivo CSV: {e}\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Arquivo CSV não encontrado.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33d248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar a existência das colunas necessárias para o cruzamento\n",
    "if 'referencia_tratada' in df.columns and 'NumeroBacen' in df_cambio.columns:\n",
    "    # Converter as colunas para o mesmo tipo (string)\n",
    "    df['referencia_tratada'] = df['referencia_tratada'].astype(str)\n",
    "    \n",
    "    # Adicionar novas colunas ao DataFrame principal\n",
    "    novas_colunas = [\"Recuperado_(S/N)\", \"Valor_recuperado\", \"Contabilizado_em_perdas_(S/N)\", \"Valor_de_perda\",\"Colaborador_Edição\", \"Colaborador_Edição_Disciplina\", \"Colaborador_Legitimação\" , \"Colaborador_Legitimação_Disciplina\"]\n",
    "    for coluna in novas_colunas:\n",
    "        df[coluna] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c433da89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Referencia_1', 'Referencia_2', 'GIN', 'referencia_tratada',\n",
       "       'Referencia_4', 'Setor', 'Valor', 'Área', 'Moeda', 'Operacao', 'Origem',\n",
       "       'Agente', 'Data_da_Pendência', 'Dias', 'Justificativa',\n",
       "       'Recuperado_(S/N)', 'Valor_recuperado', 'Contabilizado_em_perdas_(S/N)',\n",
       "       'Valor_de_perda', 'Colaborador_Edição', 'Colaborador_Edição_Disciplina',\n",
       "       'Colaborador_Legitimação', 'Colaborador_Legitimação_Disciplina'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a1268f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tratamento da coluna NumeroBacen\n",
    "df_cambio['NumeroBacen']=df_cambio['NumeroBacen'].astype(int)\n",
    "df['Referencia_4']=df['Referencia_4'].astype(str).str.replace('.','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ca6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as colunas para o mesmo tipo (string, por exemplo)\n",
    "df['referencia_tratada'] = df['referencia_tratada'].astype(str)\n",
    "df_cambio['NumeroBacen'] = df_cambio['NumeroBacen'].astype(str)\n",
    "\n",
    "# Verificar se os valores de df['referencia_tratada'] estão em df_cambio['NumeroBacen']\n",
    "df['status'] = df['referencia_tratada'].apply(lambda x: 'Ok' if x in df_cambio['NumeroBacen'].values else 'NOK')\n",
    "\n",
    "# Realizar o merge para adicionar as colunas do df_cambio\n",
    "df = df.merge(df_cambio[['NumeroBacen', 'GrpTpPessoa', 'Segmento', 'SegmentoGrupo', \n",
    "                                'NomeBeneficiarioExterior', 'NomeClienteCustodiante', \n",
    "                                ]], \n",
    "                     left_on='referencia_tratada', right_on='NumeroBacen', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d01fb008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\\\POWERBI\\\\Perdas\\\\Validação\\Valid.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Definir o diretório e o nome do arquivo de saída\n",
    "diretorio_saida = r'C:\\\\POWERBI\\\\Perdas\\\\Validação'\n",
    "nome_arquivo_saida = 'Valid.xlsx'\n",
    "caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# Criar o diretório se não existir\n",
    "os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "df.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eac23419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     referencia_tratada                                      Justificativa\n",
      "0             444056654  DashPerdas - JABIL DO BRASIL INDUSTRIA ELETROE...\n",
      "1             444056654  10/01/25 – - Solicitado devolução junto ao ban...\n",
      "2             444056654  13/01/25 – – E-mail ao cliente pedindo auxílio...\n",
      "3             444056654  14/01/25 – - Cliente informa que irá contatar ...\n",
      "4             444056654  28/01/25 – - E-mail ao cliente questionando se...\n",
      "5             444056654  06/02/25 – - Questionamos cliente quanto conta...\n",
      "6             444056654  14/02/25 – - Houve fechamento a este beneficiá...\n",
      "7             444056654  24/02/25 – - Encaminhamos e-mail ao cliente pe...\n",
      "8             434698127  DashPerdas - RC9 COMERCIO E IMPORTACAO EIRELI ...\n",
      "9             434698127   16/09 – - Solicitado devolução junto a investig.\n",
      "10            434698127  17/10 – - E-mail para investig. para verificar...\n",
      "11            434698127  23/10 – - E-mail ao cliente  solicitando auxíl...\n",
      "12            434698127  05/11 – - Novo e-mail para cliente pedindo aux...\n",
      "13            434698127  25/11 – - Novo e-mail ao cliente pedindo statu...\n",
      "14            434698127  28/11 – - Conforme cliente o beneficiário não ...\n",
      "15            434698127  05/12 – - Cliente informa que o banco do benef...\n",
      "16            434698127  06/12 – - Encaminhamos e-mail ao cliente pedin...\n",
      "17            434698127                  16/12 – - Novo e-mail ao cliente.\n",
      "18            434698127  17/12 – - Conforme cliente o banco só aceita e...\n",
      "19            434698127  20/12 – - Pedimos para a investig. envio de me...\n",
      "20            434698127  26/12 – - Encaminhamos e-mail para investig. s...\n",
      "21            434698127           27/12 – - Mensagem enviada ao banqueiro.\n",
      "22            434698127  07/01 – - Localizamos mensagem do HSBC pedindo...\n",
      "23            434698127  08/01 – - Investig. informa que não possui cha...\n",
      "24            434698127  09/01 – - Informado ao cliente que estamos tra...\n",
      "25            434698127  13/01 – - Encaminhamos e-mail para Adu para ve...\n",
      "26            434698127  15/01/25 – - Pedimos auxílio ao jurídico para ...\n",
      "27            434698127             17/01/25 – - Carta enviada ao cliente.\n",
      "28            434698127  28/01/25 – - E-mail para a empresa questionand...\n",
      "29            434698127  06/02 – - Cliente solicitou-nos os dados bancá...\n",
      "30            434698127  13/02/25 – - Questionamos cliente quanto a dev...\n",
      "31            434698127  18/02/25 – - Conforme cliente o beneficiário i...\n",
      "32            434068675  DashPerdas - RC9 COMERCIO E IMPORTACAO EIRELI ...\n",
      "33            434068675   16/09 – - Solicitado devolução junto a investig.\n",
      "34            434068675  17/10 – - E-mail para investig. para verificar...\n",
      "35            434068675  23/10 – - E-mail ao cliente solicitando auxíli...\n",
      "36            434068675  05/11 – - Novo e-mail para cliente pedindo aux...\n",
      "37            434068675  25/11 – - Novo e-mail ao cliente pedindo statu...\n",
      "38            434068675  28/11 – - Conforme cliente o beneficiário não ...\n",
      "39            434068675                  16/12 – - Novo e-mail ao cliente.\n",
      "40            434068675  17/12 – - Conforme cliente o banco só aceita e...\n",
      "41            434068675  20/12 – - Pedimos para a investig. envio de me...\n",
      "42            434068675  26/12 – - Encaminhamos e-mail para investig. s...\n",
      "43            434068675           27/12 – - Mensagem enviada ao banqueiro.\n",
      "44            434068675  30/12 – - Cliente apresentou invoice para comp...\n",
      "45            434068675  08/01 – - Questionamos cliente quanto a previs...\n",
      "46            434068675  09/01 – - Informamos ao cliente quanto ao proc...\n",
      "47            434068675  13/01 – - Cliente questionou quanto ao process...\n",
      "48            434068675  23/01 – - Encaminhamos e-mail para a empresa q...\n",
      "49            434068675  31/01/25 – - Contato telefônico com Rachel Ass...\n",
      "50            434068675  06/02/25 – - Questionamos Tayna e Rachel da RC...\n",
      "51            434068675  18/02/25 – - Conforme cliente (Rachel Assuite)...\n",
      "52            434068675  19/02/25 – - Pedimos para a investig verificar...\n",
      "53            434068675  24/02/25 – - Encaminhado cópia da mensagem env...\n",
      "54            460749431  DashPerdas - KILLING S.A. TINTAS E ADESIVOS - ...\n",
      "55            460749431  22/01/25 – - Envio de mensagem ao banqueiro no...\n",
      "56            460749431  28/01/25 – - E-mail ao trader questionando se ...\n",
      "57            460749431  06/02/25 – - Trader encaminhou e-mail ao clien...\n",
      "58            460749431  17/02/25 – - Novo e-mail ao trader para verifi...\n",
      "59            460749431  19/02/25 – - Questionamos trader quanto possív...\n",
      "60            460749431                                       14/02/25 – .\n",
      "61            460749431  24/02/25 – - Conforme trader empresa possui no...\n",
      "62            468706844  DashPerdas: DALILA TEXTIL LTDA - Pagamento efe...\n",
      "63            468706844  20/02/25 – - Envio de mensagem ao banco no ext...\n",
      "64            468706844  21/02/25 – - Pedimos ao trader (Joao Victor Cu...\n",
      "65            468706844  DashPerdas: DALILA TEXTIL LTDA - Pagamento efe...\n",
      "66            468706844  20/02/25 – - Envio de mensagem ao banco no ext...\n",
      "67            468706844  21/02/25 – - Pedimos ao trader (Joao Victor Cu...\n",
      "68            444674850  DashPerdas - FLYTOUR EVENTOS E TURISMO LTDA - ...\n",
      "69            444674850  08/11/24 – - Envio de mensagem ao banqueiro pe...\n",
      "70            444674850  25/11 – - E-mail para investig. verificar stat...\n",
      "71            444674850  27/11 – - Investig encaminhou mensagem ao banq...\n",
      "72            444674850  05/12 – - E-mail ao trader (Alexandre) pedindo...\n",
      "73            444674850  12/12 – - Novo e-mail ao trader questionando c...\n",
      "74            444674850  20/12 – - E-mail para trader pedindo cópia do ...\n",
      "75            444674850  24/12 – - Conforme retorno do trader/cliente o...\n",
      "76            444674850                                       26/12/24 – .\n",
      "77            444674850  02/01/25 – - Informamos ao trader que não loca...\n",
      "78            444674850  07/01 – - Novo e-mail ao cliente pedindo cópia...\n",
      "79            444674850  14/01 – - Trader acionou o cliente novamente p...\n",
      "80            444674850  15/01/25 – - Cliente informa que contatou o be...\n",
      "81            444674850  23/01/25 – - E-mail ao cliente pedindo para ve...\n",
      "82            444674850  28/01/25 – - Cliente solicitou os dados bancár...\n",
      "83            444674850  06/02/25 – - Recebemos em cópia e-mail interno...\n",
      "84            444674850  13/02/25 – - Encaminhamos e-mail ao cliente qu...\n",
      "85            444674850  19/02/25 – - E-mail ao trader pedindo auxílio ...\n",
      "86            468706844  DashPerdas: DALILA TEXTIL LTDA - Pagamento efe...\n",
      "87            468706844  20/02/25 – - Envio de mensagem ao banco no ext...\n",
      "88            468706844  21/02/25 – - Pedimos ao trader (Joao Victor Cu...\n",
      "89            451057325  DashPerdas - MG2 MARMORES E GRANITOS LTDA - Pa...\n",
      "90            451057325  23/12 – - Solicitação para a investig. envio d...\n",
      "91            451057325  26/12 – - E-mail ao cliente pedindo auxílio no...\n",
      "92            451057325  07/01 – - E-mail ao cliente pedindo status/aux...\n",
      "93            451057325  20/01 – - E-mail ao cliente questionando quant...\n",
      "94            451057325  22/01 – - Cliente encaminhou o valor e parcela...\n",
      "95            451057325  28/01/25 – - E-mail ao Paulo Sergio (ABRASTONE...\n",
      "96            451057325  06/02/25 – - E-mail ao cliente informando que ...\n",
      "97            451057325  13/02/25 – - Questionamos quanto devolução e p...\n",
      "98            451057325  14/02/25 – - Conforme cliente (Andressa) irá v...\n",
      "99            451057325  19/02/25 – - Novo ew-mail pédindo Status de de...\n",
      "100           451057325  25/02/25 – - Solicitamos auxílio do trader (Jo...\n",
      "101           433960146  DashPerdas - RC9 COMERCIO E IMPORTACAO EIRELI ...\n",
      "102           433960146   16/09 – - Solicitado devolução junto a investig.\n",
      "103           433960146  17/10 – - E-mail para investig. para verificar...\n",
      "104           433960146  23/10 – - E-mail ao cliente  solicitando auxíl...\n",
      "105           433960146  05/11 – - Novo e-mail para cliente pedindo aux...\n",
      "106           433960146  25/11 – - Novo e-mail ao cliente pedindo statu...\n",
      "107           433960146  28/11 – - Conforme cliente o beneficiário não ...\n",
      "108           433960146  05/12 – - Cliente informa que o banco do benef...\n",
      "109           433960146  06/12 – - Encaminhamos e-mail ao cliente pedin...\n",
      "110           433960146                  16/12 – - Novo e-mail ao cliente.\n",
      "111           433960146  17/12 – - Conforme cliente o banco só aceita e...\n",
      "112           433960146  20/12 – - Pedimos para a investig. envio de me...\n",
      "113           433960146  26/12 – - Encaminhamos e-mail para investig. s...\n",
      "114           433960146           27/12 – - Mensagem enviada ao banqueiro.\n",
      "115           433960146  07/01 – - Localizamos mensagem do HSBC pedindo...\n",
      "116           433960146  08/01 – - Investig. informa que não possui cha...\n",
      "117           433960146  09/01 – - Informado ao cliente que estamos tra...\n",
      "118           433960146  13/01 – - Encaminhamos e-mail para Adu para ve...\n",
      "119           433960146  15/01/25 – - Pedimos auxílio ao jurídico para ...\n",
      "120           433960146             17/01/25 – - Carta enviada ao cliente.\n",
      "121           433960146  28/01/25 – - E-mail para a empresa questionand...\n",
      "122           433960146  06/02 – - Cliente solicitou-nos os dados bancá...\n",
      "123           433960146  13/02/25 – - Questionamos cliente quanto a dev...\n",
      "124           433960146  18/02/25 – - Conforme cliente o beneficiário i...\n",
      "125           433960146  21/02/25 – - Cliente pediu confirmação dos dad...\n",
      "126           457661596  DashPerdas - 3 M DO BRASIL LTDA - Pagamento ef...\n",
      "127           457661596  06/01 – – E-mail para Investig. solicitando de...\n",
      "128           457661596  07/01 – - E-mail ao cliente pedindo auxílio no...\n",
      "129           457661596  08/01 – - Cliente informa que já contatou o be...\n",
      "130           457661596  20/01 – - E-mail ao cliente questionando quant...\n",
      "131           457661596  28/01/25 – - Cliente informou que está cobrand...\n",
      "132           457661596  04/02/25 – - Cliente pediu esclarecimentos qua...\n",
      "133           457661596  13/02/25 – - Questionamos cliente quanto a dev...\n",
      "134           457661596  19/02/25 – - E-mail ao trader (Renato Melo Yke...\n",
      "135           457661596                          21/02/25 – - Novo e-mail.\n",
      "136           457661596  24/02/25 – - Trader (Renato) informa que irá c...\n",
      "137           461650179  DashPerdas - YANGZI BRASIL CORPORATION S/A - P...\n",
      "138           461650179  24/01/25 – – Solicitado devolução parcial junt...\n",
      "139           461650179  27/01/25 – - Conforme trader (Jeanderson) o va...\n",
      "140           460222465  DashPerdas - WURTH INDUSTRY BRASIL LTDA. - Pag...\n",
      "141           460222465  22/01/25 – - Envio de mensagem ao banqueiro no...\n",
      "142           460222465     06/02/25 – - Encaminhamos e-mail ao cliente em\n",
      "143           460222465                   05/02/25 – e cliente retornou em\n",
      "144           460222465  06/02/25 – informando que contatou o responsáv...\n",
      "145           460222465  19/02/25 – - Encaminhamos e-mail ao trader da ...\n",
      "146           460222465  20/02/25 – - Trader Robson informa que contato...\n",
      "147           454095060  DashPerdas - 3 M DO BRASIL LTDA - Pagamento ef...\n",
      "148           454095060  06/01 – – E-mail para Investig. solicitando de...\n",
      "149           454095060  07/01 – - E-mail ao cliente pedindo auxílio no...\n",
      "150           454095060  08/01 – - Cliente informa que já contatou o be...\n",
      "151           454095060  20/01 – - E-mail ao cliente questionando quant...\n",
      "152           454095060  28/01/25 – - Cliente informou que está cobrand...\n",
      "153           454095060  04/02/25 – - Cliente pediu esclarecimentos qua...\n",
      "154           454095060  13/02/25 – - Questionamos cliente quanto a dev...\n",
      "155           454095060  19/02/25 – - E-mail ao trader (Renato Melo Yke...\n",
      "156           454095060                          21/02/25 – - Novo e-mail.\n",
      "157           454095060  24/02/25 – - Trader (Renato) informa que irá c...\n",
      "158           457461100  DashPerdas - UNIVERSAL ELECTRONICS DO BRASIL L...\n",
      "159           457461100  10/01/25 – - Envio de mensagem pedindo devoluç...\n",
      "160           457461100  14/01/25 – - Cliente pediu orientações de como...\n",
      "161           457461100  16/01/25 – - Cliente pediu os dados bancários ...\n",
      "162           457461100  20/01 – - Questionado cliente se OK quanto aos...\n",
      "163           457461100  27/01/25 – - Conforme cliente o valor será dev...\n",
      "164           457461100     06/02/25 – - Encaminhamos e-mail ao cliente em\n",
      "165           457461100  05/02/25 – questionando quanto a devolução e c...\n",
      "166           457461100  18/02/25 – - Encaminhamos e-mail ao trader e c...\n",
      "167           457461100  24/02/25 – - E-mail ao cliente questionando qu...\n",
      "168           464720658  DashPerdas: ITATIAIA ELETRO E MOVEIS S A - Pag...\n",
      "169           464720658  11/02/25 – - Envio de mensagem ao banco no ext...\n",
      "170           464720658  14/02/25 – - Segundo cliente o beneficiário já...\n",
      "171             4045747  DashPerdas - FOCUS COMERCIAL IMPORTADORA E EXP...\n",
      "172             4045747  30/12 – - E-mail ao trader pedindo auxílio no ...\n",
      "173             4045747  07/01 – - E-mail questionando trader contato c...\n",
      "174             4045747  20/01 – - Novo e-mail ao trader que acionou o ...\n",
      "175             4045747  31/01/25 – - Conforme trader o cliente irá fec...\n",
      "176             4045747              07/02/25 – para compensação do valor.\n",
      "177             4045747  17/02/25 – - E-mail ao trader questionando qua...\n",
      "178           466567515  DashPerdas: SANOFI MEDLEY FARMACEUTICA LTDA - ...\n",
      "179           466567515  21/02/25 – - Envio de mensagem ao banco no ext...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_excel(rf\"C:\\\\POWERBI\\\\Perdas\\\\Validação\\Valid.xlsx\")\n",
    "#df = pd.read_excel(rf\"C:\\Users\\t788066\\Downloads\\Valid.xlsx\")\n",
    "\n",
    "# Função para separar justificativas por data (com formatos dd/mm/yy e dd/mm)\n",
    "def separar_por_data(row):\n",
    "    justificativa = row['Justificativa']\n",
    "    datas = re.findall(r'\\d{2}/\\d{2}(?:/\\d{2})?', justificativa)\n",
    "    partes = re.split(r'\\d{2}/\\d{2}(?:/\\d{2})?', justificativa)\n",
    "    \n",
    "    novas_linhas = []\n",
    "\n",
    "    # Adicionar a primeira parte que não tem data\n",
    "    if partes[0].strip():\n",
    "        novas_linhas.append((row['referencia_tratada'], partes[0].strip()))\n",
    "\n",
    "    # Adicionar as partes que têm data\n",
    "    for i, data in enumerate(datas):\n",
    "        nova_justificativa = data + ' – ' + partes[i+1].strip()\n",
    "        novas_linhas.append((row['referencia_tratada'], nova_justificativa))\n",
    "        \n",
    "    return novas_linhas\n",
    "\n",
    "# Aplicar a função ao dataframe\n",
    "novas_linhas = df.apply(separar_por_data, axis=1).explode().tolist()\n",
    "\n",
    "# Criar um novo dataframe com as linhas separadas\n",
    "novo_df = pd.DataFrame(novas_linhas, columns=['referencia_tratada', 'Justificativa'])\n",
    "\n",
    "print(novo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a675f2",
   "metadata": {},
   "source": [
    "### Definir o diretório e o nome do arquivo de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dffc693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de casos na historico: 14\n",
      "Total de casos novos: 4\n",
      "Total de casos na historico depois da add novos casos: 18\n"
     ]
    }
   ],
   "source": [
    "# Caminho para a pasta base especificada\n",
    "caminho_base = r\"C:\\\\POWERBI\\\\Perdas\\\\Validação\\Valid.xlsx\"\n",
    "caminho_historico = rf'\\\\Fscorppawbr18\\orca_dados\\Automação_Perdas\\Base\\Historico_Perdas.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# Cria um DataFrame com os nomes dos arquivos\n",
    "df = pd.read_excel(caminho_base)\n",
    "# Lê o DataFrame histórico\n",
    "df_historico = pd.read_excel(caminho_historico, dtype=str)\n",
    "\n",
    "df = df.astype(str)\n",
    "df_historico = df_historico.astype(str)\n",
    "\n",
    "df_novos_dados = df[~df['GIN'].isin(df_historico['GIN'])]\n",
    "\n",
    "# Adiciona os novos dados ao DataFrame histórico\n",
    "df_historico_atualizado = pd.concat([df_historico, df_novos_dados], ignore_index=True)\n",
    "\n",
    "# Exibe os novos dados que foram adicionados ao histórico\n",
    "print(\"Total de casos na historico: \"+ str(df_historico.shape[0]))\n",
    "print(\"Total de casos novos: \"+ str(df_novos_dados.shape[0]))\n",
    "print(\"Total de casos na historico depois da add novos casos: \"+ str(df_historico_atualizado.shape[0]))\n",
    "\n",
    "# Salvando o DataFrame com os novos dados para ser enviado ao SharePoint\n",
    "df_novos_dados.to_excel(caminho_base, index=False)\n",
    "\n",
    "# Salva o DataFrame histórico atualizado\n",
    "df_historico_atualizado.to_excel(caminho_historico, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79b0efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Pendencias\\Valid.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Carregar o DataFrame original\n",
    "df = pd.read_excel(rf\"C:\\\\POWERBI\\\\Perdas\\\\Validação\\Valid.xlsx\")\n",
    "\n",
    "# Função para separar justificativas por data (com formatos dd/mm/yy e dd/mm)\n",
    "def separar_por_data(row):\n",
    "    justificativa = row['Justificativa']\n",
    "    datas = re.findall(r'\\d{2}/\\d{2}(?:/\\d{2})?', justificativa)\n",
    "    partes = re.split(r'\\d{2}/\\d{2}(?:/\\d{2})?', justificativa)\n",
    "    \n",
    "    novas_linhas = []\n",
    "\n",
    "    # Adicionar a primeira parte que não tem data\n",
    "    if partes[0].strip():\n",
    "        novas_linhas.append((row['referencia_tratada'], partes[0].strip()))\n",
    "\n",
    "    # Adicionar as partes que têm data\n",
    "    for i, data in enumerate(datas):\n",
    "        nova_justificativa = data + ' – ' + partes[i+1].strip()\n",
    "        novas_linhas.append((row['referencia_tratada'], nova_justificativa))\n",
    "        \n",
    "    return novas_linhas\n",
    "\n",
    "# Aplicar a função ao dataframe\n",
    "novas_linhas = df.apply(separar_por_data, axis=1).explode().tolist()\n",
    "\n",
    "# Criar um novo dataframe com as linhas separadas\n",
    "novo_df = pd.DataFrame(novas_linhas, columns=['referencia_tratada', 'Justificativa'])\n",
    "\n",
    "# Substituir as colunas no DataFrame original\n",
    "df['referencia_tratada'] = novo_df['referencia_tratada']\n",
    "df['Justificativa'] = novo_df['Justificativa']\n",
    "\n",
    "# Definir o diretório e o nome do arquivo de saída\n",
    "diretorio_saida = rf'C:\\\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Pendencias'\n",
    "nome_arquivo_saida = 'Valid.xlsx'\n",
    "caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# Criar o diretório se não existir\n",
    "os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "df.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f057849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir o diretório e o nome do arquivo de saída\n",
    "# diretorio_saida = rf'C:\\\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Pendencias'\n",
    "# nome_arquivo_saida = 'Valid.xlsx'\n",
    "# caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# # Criar o diretório se não existir\n",
    "# os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# # Salvar o DataFrame em um arquivo Excel\n",
    "# df.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "# print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed6568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1de46b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openpyxl\n",
    "# from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    " \n",
    "# # Caminho do arquivo Excel\n",
    "# file_path = rf'C:\\\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Pendencias'\n",
    " \n",
    " \n",
    "# # Carregar o arquivo Excel\n",
    "# wb = openpyxl.load_workbook(file_path)\n",
    " \n",
    "# # Selecionar a planilha desejada (por exemplo, a primeira planilha)\n",
    "# ws = wb.active\n",
    " \n",
    "# # Definir o intervalo de dados (por exemplo, de A1 até a última célula preenchida)\n",
    "# # Supondo que os dados começam na célula A1 e a primeira linha contém os cabeçalhos\n",
    "# min_col = ws.min_column\n",
    "# min_row = ws.min_row\n",
    "# max_col = ws.max_column\n",
    "# max_row = ws.max_row\n",
    " \n",
    "# # Definir o intervalo de dados\n",
    "# data_range = f\"A{min_row}:{openpyxl.utils.get_column_letter(max_col)}{max_row}\"\n",
    " \n",
    "# # Criar a tabela\n",
    "# table = Table(displayName=\"Table1\", ref=data_range)\n",
    " \n",
    "# # Adicionar estilo à tabela\n",
    "# style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "#                        showLastColumn=False, showRowStripes=True, showColumnStripes=True)\n",
    "# table.tableStyleInfo = style\n",
    " \n",
    "# # Adicionar a tabela à planilha\n",
    "# ws.add_table(table)\n",
    " \n",
    "# # Salvar o arquivo Excel\n",
    "# wb.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca949f",
   "metadata": {},
   "source": [
    "### Salvar em tabela sharepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "631c993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Worksheet \"Sheet1\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import openpyxl\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    " \n",
    "# Caminho do arquivo Excel\n",
    "file_path = r'C:\\\\Users\\\\T694816\\\\Santander Office 365\\\\IO-Controle Perdas e Pendencias - Documentos\\\\Pendencias\\\\Valid.xlsx'\n",
    " \n",
    " \n",
    "# Carregar o arquivo Excel\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    " \n",
    "# Selecionar a planilha desejada (por exemplo, a primeira planilha)\n",
    "ws = wb.active\n",
    "\n",
    "display(ws)\n",
    " \n",
    "# Definir o intervalo de dados (por exemplo, de A1 até a última célula preenchida)\n",
    "# Supondo que os dados começam na célula A1 e a primeira linha contém os cabeçalhos\n",
    "min_col = ws.min_column\n",
    "min_row = ws.min_row\n",
    "max_col = ws.max_column\n",
    "max_row = ws.max_row\n",
    " \n",
    "# Definir o intervalo de dados\n",
    "data_range = f\"A{min_row}:{openpyxl.utils.get_column_letter(max_col)}{max_row}\"\n",
    " \n",
    "# Criar a tabela\n",
    "table = Table(displayName=\"Table1\", ref=data_range)\n",
    " \n",
    "# Adicionar estilo à tabela\n",
    "style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "                       showLastColumn=False, showRowStripes=True, showColumnStripes=True)\n",
    "table.tableStyleInfo = style\n",
    " \n",
    "# Adicionar a tabela à planilha\n",
    "ws.add_table(table)\n",
    " \n",
    "# Salvar o arquivo Excel\n",
    "wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf604f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir o diretório e o nome do arquivo de saída\n",
    "# diretorio_saida = r'C:\\\\POWERBI\\\\Perdas\\\\Validação'\n",
    "# nome_arquivo_saida = 'Valid.xlsx'\n",
    "# caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# # Criar o diretório se não existir\n",
    "# os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# # Salvar o DataFrame em um arquivo Excel\n",
    "# df.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "# print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ac3525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV carregado com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# Definir o diretório e o nome do arquivo\n",
    "diretorio = r'C:\\\\POWERBI\\\\Perdas\\\\Bases'\n",
    "nome_arquivo2 = 'P16-Back_3_202412.csv'\n",
    "caminho_arquivo2 = os.path.join(diretorio, nome_arquivo2)\n",
    "\n",
    "# Verificar se o arquivo CSV existe\n",
    "if os.path.exists(caminho_arquivo2):\n",
    "    # Carregar o arquivo CSV em um DataFrame sem cabeçalho\n",
    "    df_back = pd.read_csv(caminho_arquivo2, header=None, sep=';')\n",
    "    print(\"Arquivo CSV carregado com sucesso.\")\n",
    "    \n",
    "    # Selecionar as colunas de índice 2, 7, 9, 12 e 22\n",
    "    colunas_indices = [2, 5, 9, 11, 22]\n",
    "    colunas_nomes = [\n",
    "        'Codigo_Evento_Area', 'Tipo_de_Dado_Financeiro', 'Data_contabilização', \n",
    "        'Valor_do_lançamento', 'Descrição_do_evento'\n",
    "    ]\n",
    "\n",
    "    df_back = df_back.iloc[:, colunas_indices]\n",
    "    df_back.columns = colunas_nomes  # Renomear as colunas\n",
    "    \n",
    "    # Função para extrair o comentário específico\n",
    "    def extract_comment(text):\n",
    "        match = re.search(r'(Pagamento efetuado a beneficiário incorreto.*?)(?=\\d{2}/\\d{2}/\\d{2})', text)\n",
    "        return match.group(1).strip() if match else None\n",
    "\n",
    "    # Aplicar a função para criar a nova coluna de comentário\n",
    "    df_back['Comentario'] = df_back['Descrição_do_evento'].apply(extract_comment)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1cbfff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir o diretório e o nome do arquivo\n",
    "# diretorio = r'C:\\\\POWERBI\\\\Perdas\\\\Bases'\n",
    "# nome_arquivo2 = 'P16-Back_3_202412.csv'\n",
    "# caminho_arquivo2 = os.path.join(diretorio, nome_arquivo2)\n",
    "\n",
    "# # Verificar se o arquivo CSV existe\n",
    "# if os.path.exists(caminho_arquivo2):\n",
    "#     # Carregar o arquivo CSV em um DataFrame sem cabeçalho\n",
    "#     df_back = pd.read_csv(caminho_arquivo2, header=None, sep=';')\n",
    "#     print(\"Arquivo CSV carregado com sucesso.\")\n",
    "    \n",
    "#     # Selecionar as colunas de índice 2, 6, 10, 12 e 23\n",
    "#     colunas_indices = [2, 7, 9, 12, 22]\n",
    "#     colunas_nomes = [\n",
    "#         'Codigo_Evento_Area', 'Tipo_de_Dado_Financeiro', 'Data_contabilização', \n",
    "#         'Valor_do_lançamento', 'Descrição_do_evento'\n",
    "#     ]\n",
    "\n",
    "#     df_back = df_back.iloc[:, colunas_indices]\n",
    "#     df_back.columns = colunas_nomes  # Renomear as colunas\n",
    "    \n",
    "# #     # Exibir as primeiras linhas do DataFrame reduzido\n",
    "# #     print(\"Primeiras linhas do DataFrame reduzido:\")\n",
    "# #     print(df_back.head())\n",
    "# # else:\n",
    "# #     print(\"Arquivo CSV não encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71739744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar novas colunas ao DataFrame\n",
    "novas_colunas = [\n",
    "    \"Sistema\", \"Colaborador_edição\", \"Medida_disciplinar_edição\", \"Colaborador_da_legitimação\", \"Medida_disciplinar_legit\",\n",
    "    \"Chances_de_recuperar_o_valor\", \"Status_da_Recuperação\", \"Observações\", \"Previsão_de_regularização\",\"GIN\"\n",
    "]\n",
    "for coluna in novas_colunas:\n",
    "    df_back[coluna] = None\n",
    "\n",
    "# # Exibir os nomes das colunas após adicionar novas colunas\n",
    "# print(\"Nomes das colunas após adicionar novas colunas:\")\n",
    "# print(df_back.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17f59416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter as colunas para o mesmo tipo (string, por exemplo)\n",
    "df_back['Codigo_Evento_Area'] = df_back['Codigo_Evento_Area'].astype(str)\n",
    "df_cambio['NumeroBacen'] = df_cambio['NumeroBacen'].astype(str)\n",
    "\n",
    "# Verificar se os valores de df_back['Codigo_Evento_Area'] estão em df_cambio['NumeroBacen']\n",
    "df_back['status'] = df_back['Codigo_Evento_Area'].apply(lambda x: 'Ok' if x in df_cambio['NumeroBacen'].values else 'NOK')\n",
    "\n",
    "# Realizar o merge para adicionar as colunas do df_cambio para os casos 'Ok'\n",
    "df_back_ok = df_back[df_back['status'] == 'Ok']\n",
    "df_back_ok = df_back_ok.merge(df_cambio[['NumeroBacen', 'GrpTpPessoa', 'Segmento', 'SegmentoGrupo', \n",
    "                                         'NomeBeneficiarioExterior', 'NomeClienteCustodiante', \n",
    "                                         ]], \n",
    "                              left_on='Codigo_Evento_Area', right_on='NumeroBacen', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "624c6c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de casos na historico: 189\n",
      "Total de casos novos: 4\n",
      "Total de casos na historico depois da add novos casos: 193\n"
     ]
    }
   ],
   "source": [
    "# Caminho para a pasta base especificada\n",
    "caminho_base = r\"C:\\\\POWERBI\\\\Perdas\\\\Validação\\Valid.xlsx\"\n",
    "caminho_historico = rf'\\\\Fscorppawbr18\\orca_dados\\Automação_Perdas\\Base\\Historico_IRO.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "# Cria um DataFrame com os nomes dos arquivos\n",
    "df = pd.read_excel(caminho_base)\n",
    "# Lê o DataFrame histórico\n",
    "df_historico = pd.read_excel(caminho_historico, dtype=str)\n",
    "\n",
    "df = df.astype(str)\n",
    "df_historico = df_historico.astype(str)\n",
    "\n",
    "df_novos_dados = df[~df['NumeroBacen'].isin(df_historico['NumeroBacen'])]\n",
    "\n",
    "# Adiciona os novos dados ao DataFrame histórico\n",
    "df_historico_atualizado = pd.concat([df_historico, df_novos_dados], ignore_index=True)\n",
    "\n",
    "# Exibe os novos dados que foram adicionados ao histórico\n",
    "print(\"Total de casos na historico: \"+ str(df_historico.shape[0]))\n",
    "print(\"Total de casos novos: \"+ str(df_novos_dados.shape[0]))\n",
    "print(\"Total de casos na historico depois da add novos casos: \"+ str(df_historico_atualizado.shape[0]))\n",
    "\n",
    "# Salvando o DataFrame com os novos dados para ser enviado ao SharePoint\n",
    "df_novos_dados.to_excel(caminho_base, index=False)\n",
    "\n",
    "# Salva o DataFrame histórico atualizado\n",
    "df_historico_atualizado.to_excel(caminho_historico, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f13222",
   "metadata": {},
   "source": [
    "### Definir o diretório e o nome do arquivo de saída no C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "022b2725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\\\POWERBI\\\\Perdas\\\\Validação\\IRO.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Definir o diretório e o nome do arquivo de saída\n",
    "diretorio_saida = r'C:\\\\POWERBI\\\\Perdas\\\\Validação'\n",
    "nome_arquivo_saida = 'IRO.xlsx'\n",
    "caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# Criar o diretório se não existir\n",
    "os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "df_back_ok.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050feda2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8c05d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo salvo em: C:\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Perdas\\IRO.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Definir o diretório e o nome do arquivo de saída\n",
    "diretorio_saida = rf'C:\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Perdas'\n",
    "nome_arquivo_saida = 'IRO.xlsx'\n",
    "caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# Criar o diretório se não existir\n",
    "os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# Salvar o DataFrame em um arquivo Excel\n",
    "df_back_ok.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a1164",
   "metadata": {},
   "source": [
    "### Salvar em tabela sharepoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6def87c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Worksheet \"Sheet1\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openpyxl\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    " \n",
    "# Caminho do arquivo Excel\n",
    "file_path = r'C:\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Perdas\\\\IRO.xlsx'\n",
    " \n",
    " \n",
    "# Carregar o arquivo Excel\n",
    "wb = openpyxl.load_workbook(file_path)\n",
    " \n",
    "# Selecionar a planilha desejada (por exemplo, a primeira planilha)\n",
    "ws = wb.active\n",
    "\n",
    "display(ws)\n",
    " \n",
    "# Definir o intervalo de dados (por exemplo, de A1 até a última célula preenchida)\n",
    "# Supondo que os dados começam na célula A1 e a primeira linha contém os cabeçalhos\n",
    "min_col = ws.min_column\n",
    "min_row = ws.min_row\n",
    "max_col = ws.max_column\n",
    "max_row = ws.max_row\n",
    " \n",
    "# Definir o intervalo de dados\n",
    "data_range = f\"A{min_row}:{openpyxl.utils.get_column_letter(max_col)}{max_row}\"\n",
    " \n",
    "# Criar a tabela\n",
    "table = Table(displayName=\"Table1\", ref=data_range)\n",
    " \n",
    "# Adicionar estilo à tabela\n",
    "style = TableStyleInfo(name=\"TableStyleMedium9\", showFirstColumn=False,\n",
    "                       showLastColumn=False, showRowStripes=True, showColumnStripes=True)\n",
    "table.tableStyleInfo = style\n",
    " \n",
    "# Adicionar a tabela à planilha\n",
    "ws.add_table(table)\n",
    " \n",
    "# Salvar o arquivo Excel\n",
    "wb.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bb91fb",
   "metadata": {},
   "source": [
    "### # Definir o diretório e o nome do arquivo de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a86e0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Definir o diretório e o nome do arquivo de saída\n",
    "# diretorio_saida = rf'C:\\Users\\T694816\\Santander Office 365\\IO-Controle Perdas e Pendencias - Documentos\\Perdas'\n",
    "# nome_arquivo_saida = 'IRO.xlsx'\n",
    "# caminho_arquivo_saida = os.path.join(diretorio_saida, nome_arquivo_saida)\n",
    "\n",
    "# # Criar o diretório se não existir\n",
    "# os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "# # Salvar o DataFrame em um arquivo Excel\n",
    "# df_back_ok.to_excel(caminho_arquivo_saida, index=False)\n",
    "\n",
    "# print(f\"Arquivo salvo em: {caminho_arquivo_saida}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
